import numpy as np
import pandas as pd
import tkinter as tk
from tkinter import messagebox
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# Load the data
df = pd.read_csv("Pt.csv", names=["english", "portuguese"])  

# Tokenize English and Portuguese
english_texts = df["english"].values
portuguese_texts = df["portuguese"].values

english_tokenizer = Tokenizer()
portuguese_tokenizer = Tokenizer()

english_tokenizer.fit_on_texts(english_texts)
portuguese_tokenizer.fit_on_texts(portuguese_texts)

# Convert to sequences
english_sequences = english_tokenizer.texts_to_sequences(english_texts)
portuguese_sequences = portuguese_tokenizer.texts_to_sequences(portuguese_texts)

# Pad sequences
max_len_english = max(len(seq) for seq in english_sequences)
max_len_portuguese = max(len(seq) for seq in portuguese_sequences)

english_padded = pad_sequences(english_sequences, maxlen=max_len_english, padding="post")
portuguese_padded = pad_sequences(portuguese_sequences, maxlen=max_len_portuguese, padding="post")

# Define vocabulary sizes
num_encoder_tokens = len(english_tokenizer.word_index) + 1
num_decoder_tokens = len(portuguese_tokenizer.word_index) + 1

# Define the LSTM model
latent_dim = 64

# Encoder
encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True)
_, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation="softmax")
decoder_outputs = decoder_dense(decoder_outputs)

# Define the full model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Prepare target data
decoder_target_data = portuguese_padded[:, 1:]  # Target data is shifted Portuguese sequences

# Compile the model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# Train the model
model.fit(
    [english_padded, portuguese_padded[:, :-1]],  # Encoder and decoder inputs
    np.expand_dims(decoder_target_data, -1),     # Decoder target data with one-hot encoding
    batch_size=2,
    epochs=1,
)

# Save and load the model
model.save("s2s_model.keras")
model = tf.keras.models.load_model("s2s_model.keras")

# Define inference models
encoder_model = Model(encoder_inputs, encoder_states)

decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(
    decoder_embedding, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states
)

# Translation function
reverse_target_char_index = {i: word for word, i in portuguese_tokenizer.word_index.items()}
target_token_index = portuguese_tokenizer.word_index
max_decoder_seq_length = max_len_portuguese

def translate_sentence(input_seq):
    states_value = encoder_model.predict(input_seq, verbose=0)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_token_index.get("<start>", 1)
    stop_condition = False
    decoded_sentence = ""

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index.get(sampled_token_index, "")
        decoded_sentence += sampled_char + " "

        if sampled_char == "<end>" or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True

        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]

    return decoded_sentence.strip()

# Create GUI
def create_gui():
    def on_translate():
        english_text = input_text.get("1.0", tk.END).strip()
        if not english_text:
            messagebox.showerror("Error", "Please enter a sentence to translate.")
            return

        input_seq = english_tokenizer.texts_to_sequences([english_text])
        input_seq = pad_sequences(input_seq, maxlen=max_len_english, padding="post")
        translation = translate_sentence(input_seq)
        output_text.delete("1.0", tk.END)
        output_text.insert(tk.END, translation)

    window = tk.Tk()
    window.title("Language Translator")
    window.geometry("500x300")

    input_label = tk.Label(window, text="Enter English Sentence:")
    input_label.pack(pady=5)

    input_text = tk.Text(window, height=5, width=50)
    input_text.pack(pady=5)

    translate_button = tk.Button(window, text="Translate", command=on_translate)
    translate_button.pack(pady=5)

    output_label = tk.Label(window, text="Translated Portuguese Sentence:")
    output_label.pack(pady=5)

    output_text = tk.Text(window, height=5, width=50, state=tk.NORMAL)
    output_text.pack(pady=5)

    window.mainloop()

create_gui()
import numpy as np
import pandas as pd
import tkinter as tk
from tkinter import messagebox
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer



# Load the data
df = pd.read_csv("Pt.csv", names=["english", "portuguese"])  

# Tokenize English and Portuguese
english_texts = df["english"].values
portuguese_texts = df["portuguese"].values

english_tokenizer = Tokenizer()
portuguese_tokenizer = Tokenizer()

english_tokenizer.fit_on_texts(english_texts)
portuguese_tokenizer.fit_on_texts(portuguese_texts)

# Convert to sequences
english_sequences = english_tokenizer.texts_to_sequences(english_texts)
portuguese_sequences = portuguese_tokenizer.texts_to_sequences(portuguese_texts)

# Pad sequences
max_len_english = max(len(seq) for seq in english_sequences)
max_len_portuguese = max(len(seq) for seq in portuguese_sequences)

english_padded = pad_sequences(english_sequences, maxlen=max_len_english, padding="post")
portuguese_padded = pad_sequences(portuguese_sequences, maxlen=max_len_portuguese, padding="post")

# Add special tokens for the decoder
portuguese_padded = np.hstack(
    [
        np.full((portuguese_padded.shape[0], 1), portuguese_tokenizer.word_index.get("<start>", 0)),
        portuguese_padded,
    ]
)

# Define the LSTM model
latent_dim = 64

#Encode
encoder_inputs = keras.Input(shape=(None,))
encoder = keras.layers.LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
# We discard encoder_outputs and only keep the states.
encoder_states = [state_h, state_c]

#decode
decoder_inputs = keras.Input(shape=(None,))
# We set up our decoder to return full output sequences,
# and to return internal states as well. We don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = keras.layers.Dense(num_decoder_tokens, activation="softmax")
decoder_outputs = decoder_dense(decoder_outputs)

model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.fit(
    [english_padded,portuguese_padded],
    decoder_target_data,
    batch_size=2,
    epochs=1,
)

model.save("s2s_model.keras")

model = keras.models.load_model("s2s_model.keras")

encoder_inputs = model.input[0]  # input_1
encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1
encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model(encoder_inputs, encoder_states)

decoder_inputs = model.input[1]  # input_2
decoder_state_input_h = keras.Input(shape=(latent_dim,))
decoder_state_input_c = keras.Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_lstm = model.layers[3]
decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]
decoder_dense = model.layers[4]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states
)

def translate_sentence (input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq, verbose=0)
    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0,portuguese_tokenizer.target_token_index["\t"]] = 1.0

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ""
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value, verbose=0
        )
# Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = portuguese_tokenizer.reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if sampled_char == "\n" or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]
    return decoded_sentence

# Create the GUI
def create_gui():
    def on_translate():
        english_text = input_text.get("1.0", tk.END).strip()
        if not english_text:
            messagebox.showerror("Error", "Please enter a sentence to translate.")
            return
        translation = translate_sentence(portuguese_texts)
        output_text.delete("1.0", tk.END)
        output_text.insert(tk.END, translation)

    # Initialize the main window
    window = tk.Tk()
    window.title("Language Translator")
    window.geometry("500x300")

    # Input label and text box
    input_label = tk.Label(window, text="Enter English Sentence:")
    input_label.pack(pady=5)

    input_text = tk.Text(window, height=5, width=50)
    input_text.pack(pady=5)

    # Translate button
    translate_button = tk.Button(window, text="Translate", command=on_translate)
    translate_button.pack(pady=5)

    # Output label and text box
    output_label = tk.Label(window, text="Translated Portuguese Sentence:")
    output_label.pack(pady=5)

    output_text = tk.Text(window, height=5, width=50, state=tk.NORMAL)
    output_text.pack(pady=5)

    # Run the GUI event loop
    window.mainloop()

# Run the GUI
create_gui()
